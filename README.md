# UI Component Detector

## Overview

This project implements an AI-driven system that analyzes UI screenshots and identifies common UI components such as:

- Navigation bars
- Buttons
- Input fields
- Cards / containers
- Other meaningful UI structures

The system exposes a backend API for detection and includes an optional lightweight frontend to upload images and visualize detected components with bounding boxes.

**Goal:**  
To demonstrate architecture design, reasoning, and practical trade-offs when building an intelligent UI analysis system under real-world constraints.

---

## Architecture Overview

The system is implemented as a **multi-step computer vision pipeline**:

### 1. Input Handling
- Accepts UI screenshots as:
  - Base64 strings
  - Image URLs
- Decodes images into an OpenCV-compatible format

### 2. Preprocessing
- Adds a small border around images to avoid edge-clipping issues
- Converts images to grayscale
- Applies edge detection to highlight UI boundaries

### 3. Region Extraction
- Uses contour detection to identify visual regions
- Extracts bounding boxes for candidate UI elements
- Filters out very small or noisy regions

### 4. Heuristic Classification
- Classifies each region using layout-based heuristics
- Uses relative size and position to infer UI component types

### 5. Post-processing
- Applies class-aware and containment-aware Non-Maximum Suppression (NMS)
- Removes duplicate detections while preserving UI hierarchy

### 6. Output
- Returns structured JSON with:
  - Component type
  - Description
  - Confidence score
  - Normalized bounding box coordinates

An optional frontend is provided for interactive visualization.

---

## Why This Approach Was Chosen

Given the **48-hour timeline** and the need for **consistent, explainable bounding boxes**, a deterministic computer-vision-based approach was chosen over training or integrating large ML models.

This approach provides:

- Predictable and reproducible outputs
- No dependency on external APIs or paid services
- Full control over detection behavior
- Clear explainability of decisions

The focus was on building a **reliable baseline system**, not a black-box semantic classifier.

---

## Technical Decisions & Trade-offs

### Key Decisions
- Used classical computer vision techniques (OpenCV) instead of deep learning
- Implemented heuristic-based classification using geometry and layout rules
- Added class-aware NMS to reduce duplicate detections
- Built a minimal frontend for visualization instead of a full UI application

### Trade-offs
- Higher consistency and explainability
- Lower semantic accuracy compared to trained ML models
- UI components inferred by layout rather than true semantic understanding

These trade-offs were intentional.

---

## Model Selection Reasoning

No pre-trained or custom-trained ML models were used in this version.

Instead, the system relies on:
- Classical image processing
- Rule-based semantic inference
- A multi-step processing pipeline

This was chosen because:
- Training UI-specific models requires labeled datasets and more time
- Vision-language models introduce variability and cost
- Heuristics guarantee stable and deterministic bounding boxes

---

## Setup & Installation Instructions

### Prerequisites
- Python 3.9+
- Virtual environment recommended

### Installation

-----bash--------
git clone <repository-url>
cd ui-component-detector
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
Run the Server
uvicorn app.main:app --reload

### Access Frontend (Optional)
http://127.0.0.1:8000/ui

### API Usage Instructions
# Endpoint

POST /detect-ui-elements

Request Body
{
  "image": "<base64-string OR image-url>"
}

# Response Example
{
  "elements": [
    {
      "type": "button",
      "confidence": 0.7,
      "description": "Detected button based on layout heuristics",
      "bounds": {
        "x": 0.12,
        "y": 0.45,
        "w": 0.18,
        "h": 0.08
      }
    }
  ]
}


Bounding box values are normalized relative to image dimensions.

### Limitations

-Classification is based on visual layout, not semantic understanding

-Photo-heavy or unconventional UI designs may produce noisy detections

-Overlapping detections may still occur in complex layouts

-Components such as sliders or switches are inferred heuristically

These limitations are inherent to geometry-based approaches.

### What I Would Improve With More Time

-Integrate a UI-trained object detection model (e.g., RICO or UIED)

-Add OCR to improve text-aware classification

-Introduce hierarchical grouping of UI components

-Combine classical CV with vision-language models for semantic labeling

-Add confidence calibration and evaluation metrics

-Deploy the system as a public demo

### Samples

The repository includes:

-3â€“5 UI screenshots used for testing

-Corresponding JSON outputs generated by the system

-Visual overlay images demonstrating bounding box visualization

### Final Notes

This project demonstrates a practical, explainable, and extensible baseline for UI component detection.

The architecture is intentionally modular and can be extended with learned models without changing the API contract.
